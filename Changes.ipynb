{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiIugw2GIg9r3duhJfBFCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppfenninger/Sensorimotor_Learning_Final/blob/main/Changes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_QxjWyEGgtR"
      },
      "outputs": [],
      "source": [
        "# Imports Added\n",
        "from google.colab import files, drive\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New door Key Env\n",
        "class SizedDoorKeyEnv(DoorKeyEnv):\n",
        "    def __init__(self, size=5):\n",
        "        super().__init__(size=size)\n",
        "    \n",
        "    def _reward(self):\n",
        "        \"\"\"\n",
        "        Compute the reward to be given upon success\n",
        "        \"\"\"\n",
        "        return 1"
      ],
      "metadata": {
        "id": "ma3LfjZ9GnGR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New Config\n",
        "class Config:\n",
        "    def __init__(self,\n",
        "                score_threshold=0.93,\n",
        "                discount=0.995,\n",
        "                lr=1e-3,\n",
        "                max_grad_norm=0.5,\n",
        "                log_interval=10,\n",
        "                max_episodes=2000,\n",
        "                bernoulli_param=0.5,\n",
        "                gae_lambda=0.95,\n",
        "                num_critics=0,\n",
        "                clip_ratio=0.2,\n",
        "                target_kl=0.01,\n",
        "                train_ac_iters=5,\n",
        "                use_discounted_reward=False,\n",
        "                exploration_beta = 0.1,\n",
        "                entropy_coef=0.01,\n",
        "                use_gae=False,\n",
        "                exploration_update_freq = 10,\n",
        "                tau = .95,\n",
        "                use_soft_target_update = False, \n",
        "                seed=0, ####added\n",
        "                env_size=5 ####added\n",
        "              ):\n",
        "        \n",
        "        self.score_threshold = score_threshold\n",
        "        self.discount = discount\n",
        "        self.lr = lr\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.log_interval = log_interval\n",
        "        self.bernoulli_param = bernoulli_param\n",
        "        self.max_episodes = max_episodes\n",
        "        self.num_critics = num_critics\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.target_kl = target_kl\n",
        "        self.train_ac_iters = train_ac_iters\n",
        "        self.gae_lambda=gae_lambda\n",
        "        self.use_discounted_reward=use_discounted_reward\n",
        "        self.exploration_beta = exploration_beta\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.use_gae = use_gae\n",
        "        self.exploration_update_freq = exploration_update_freq\n",
        "        self.tau = tau\n",
        "        self.use_soft_target_update = use_soft_target_update\n",
        "        self.seed=seed #### added\n",
        "        self.env_size=env_size #### added"
      ],
      "metadata": {
        "id": "yYtdPsPCGqCl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed environment used in run_experiment - line 8 is the only one I changed\n",
        "def run_experiment(args, parameter_update, seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = SizedDoorKeyEnv(args.env_size) ####CHANGED LINE\n",
        "\n",
        "    acmodel = ACModel(env.action_space.n, num_critics=args.num_critics)\n",
        "    target_acmodel = ACModel(env.action_space.n, num_critics=args.num_critics)\n",
        "    # acmodel.move_to_device(device)\n",
        "    acmodel.to(device)\n",
        "    target_acmodel.to(device)\n",
        "\n",
        "    is_solved = False\n",
        "    \n",
        "    SMOOTH_REWARD_WINDOW = 50\n",
        "\n",
        "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
        "    \n",
        "    optimizer = torch.optim.Adam(acmodel.parameters(), lr=args.lr)\n",
        "    num_frames = 0\n",
        "\n",
        "    num_eps = 0\n",
        "    pbar = tqdm(range(args.max_episodes))\n",
        "    for update in pbar:\n",
        "        exps, logs1 = collect_experiences(env, acmodel, args, device, target_acmodel=target_acmodel)\n",
        "        logs2 = parameter_update(optimizer, acmodel, exps, args)\n",
        "        # TODO: they do it as steps but that's a little unclear what that is rn\n",
        "        # TODO: this works for the hard update but the soft update should update like \n",
        "        # every step and right now it's not updating often enough\n",
        "        if num_eps % args.exploration_update_freq == 0 : \n",
        "\n",
        "          update_target_vnet(acmodel, target_acmodel, soft=args.use_soft_target_update)\n",
        "\n",
        "        ## Use this function to caluclate states on the experience (one episode)\n",
        "        logs_stats = calculate_logging_stats(exps)\n",
        "\n",
        "\n",
        "        logs = {**logs1, **logs2, **logs_stats}\n",
        "\n",
        "        num_frames += logs[\"num_frames\"]\n",
        "        \n",
        "        rewards.append(logs[\"return_per_episode\"])\n",
        "        \n",
        "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
        "\n",
        "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
        "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"],\n",
        "                'avg_std_dev':logs[\"avg_std_dev\"]}\n",
        "        \n",
        "        if args.num_critics > 0:\n",
        "          #TODO MR: Consider logging value loss of each critic\n",
        "            data['value_loss'] = logs[\"value_loss\"]\n",
        "\n",
        "        pd_logs.append(data)\n",
        "\n",
        "        pbar.set_postfix(data)\n",
        "\n",
        "        # Early terminate\n",
        "        if smooth_reward >= args.score_threshold:\n",
        "            is_solved = True\n",
        "            break\n",
        "        num_eps += 1 \n",
        "\n",
        "    if is_solved:\n",
        "        print('Solved!')\n",
        "    \n",
        "    return pd.DataFrame(pd_logs).set_index('episode')\n",
        "\n",
        "  \n",
        "def update_target_vnet(acmodel, target_acmodel, soft=False):\n",
        "    if not soft:\n",
        "        #### TODO: update the target Q function in a \"hard\" way\n",
        "        #### copy the parameter values in self.qnet into self.target_qnet\n",
        "        target_acmodel.load_state_dict(acmodel.state_dict())\n",
        "\n",
        "    else:\n",
        "        #### TODO: soft update on taget Q network.\n",
        "        #### similar to polyak averaging, we update the target Q network slowly\n",
        "        #### $\\theta_Qtgt = \\tau*\\theta_Qtgt + (1-\\tau)*\\theta_Q\n",
        "        for param, new_param in zip(target_acmodel.parameters(), acmodel.parameters()) :\n",
        "              #  param.data =  self.tau*param.data + (1.0-self.tau)*new_param.data\n",
        "              param.data.copy_(args.tau * param.data + (1.0 - args.tau)*new_param.data)"
      ],
      "metadata": {
        "id": "sXFYtfPSGtla"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Param Loop Code\n",
        "##### ADDED CODE\n",
        "def param_loop(folder_location=\"/content/gdrive/My Drive/Sensorimotor_Final/\",\n",
        "               file_information_path= \"/content/gdrive/My Drive/Sensorimotor_Final/file_infos.pckl\",\n",
        "               exploration_betas=[0.1],\n",
        "               nums_critics=[3],\n",
        "               env_sizes=[5],\n",
        "               seeds=[0],\n",
        "               use_soft_target_update=True):\n",
        "  file_infos = []\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  for seed in seeds:\n",
        "    print(\"seed\", seed)\n",
        "    for exploration_beta in exploration_betas:\n",
        "      print(\"exploration_beta\", exploration_beta)\n",
        "      for num_critics in nums_critics:\n",
        "        print(\"num_critics\", num_critics)\n",
        "        for env_size in env_sizes:\n",
        "          print(\"env_size\", env_size)\n",
        "          args = Config(use_gae=True, num_critics=num_critics, seed=seed, exploration_beta=exploration_beta, env_size=env_size, use_soft_target_update=use_soft_target_update)\n",
        "\n",
        "          logs = run_experiment(args, update_parameters_with_baseline, seed=seed)\n",
        "\n",
        "          file_name = folder_location + \"_beta-\" + str(exploration_beta) + \"_critic-\" + str(num_critics) + \"_env-\" + str(env_size) + \"_seed-\" + str(seed) + \"_soft-\" + str(use_soft_target_update) + \".pckl\"\n",
        "          f = open(file_name, \"wb\")\n",
        "          pickle.dump(logs, f)\n",
        "          f.close()\n",
        "\n",
        "          file_info = {\"exploration_beta\": exploration_beta,\n",
        "                       \"num_critics\": num_critics,\n",
        "                       \"env_size\": env_size,\n",
        "                       \"seed\": seed,\n",
        "                       \"use_soft_target_update\": use_soft_target_update,\n",
        "                       \"file_name\": file_name\n",
        "                       }\n",
        "          file_infos.append(file_info)\n",
        "  \n",
        "          # do this every time in case something dies mid run\n",
        "          f = open(file_information_path, \"wb\")\n",
        "          pickle.dump(file_infos, f)\n",
        "          f.close()"
      ],
      "metadata": {
        "id": "3B4-wxB0G6n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to read data from a file_info file\n",
        "def read_files(file_infos_path):\n",
        "  f = open(file_infos_path, \"rb\")\n",
        "  file_infos = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  for file_info in file_infos:\n",
        "    file_name = file_info[\"file_name\"]\n",
        "\n",
        "    f = open(file_name, \"rb\")\n",
        "    data = pickle.load(f)\n",
        "    f.close\n",
        "\n",
        "    file_info[\"data\"] = data\n",
        "\n",
        "  return file_infos\n",
        "\n",
        "  file_infos = read_files(file_infos_path)"
      ],
      "metadata": {
        "id": "Wbz7psFpHFDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "param_loop()\n",
        "file_infos = read_files(\"/content/gdrive/My Drive/Sensorimotor_Final/file_infos.pckl\")\n",
        "file_infos[0][\"data\"].plot(x='num_frames', y=['reward', 'smooth_reward'])"
      ],
      "metadata": {
        "id": "C3R7wkOfHJ3v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}